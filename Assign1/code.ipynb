{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from mnist import MNIST\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "mndata = MNIST('./data')\n",
    "train_valid_images, train_valid_labels = mndata.load_training()\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "train_valid_images, train_valid_labels = np.array(train_valid_images), np.array(train_valid_labels)\n",
    "test_images, test_labels = np.array(test_images), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, valid_images, train_labels, valid_labels = train_test_split(train_valid_images, train_valid_labels, test_size=0.3, random_state=0, stratify=train_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练，两层全连接神经网络\n",
    "# 激活函数\n",
    "# 反向传播，loss以及梯度的计算\n",
    "# 学习率下降策略\n",
    "# L2正则化\n",
    "# 优化器SGD\n",
    "# 保存模型\n",
    "\n",
    "# 模型使用 Linear -> ReLu -> Linear -> CrossEntropy -> SGD\n",
    "\n",
    "class Linear():\n",
    "    def __init__(self, input_dim, output_dim, bias=True):\n",
    "        self._gradient = None\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.bias = bias\n",
    "\n",
    "        self.w = np.random.uniform(-np.sqrt(1 / self.input_dim), np.sqrt(1 / self.input_dim), [input_dim, output_dim]) # input_dim * output_dim\n",
    "        self.b = np.random.uniform(-np.sqrt(1 / self.input_dim), np.sqrt(1 / self.input_dim), [1, output_dim])\n",
    "        if bias:\n",
    "            self.w = np.vstack((self.w, self.b.reshape(1, -1)))\n",
    "        self._parameters_dict = None\n",
    "\n",
    "    def __call__(self, X, update_grad=False):\n",
    "        num_sample, num_feature = X.shape\n",
    "        if self.bias:\n",
    "            num_feature += 1\n",
    "            X = np.hstack((X, np.ones(num_sample).reshape(-1, 1))) # num_sample * input_dim\n",
    "        \n",
    "        if update_grad:\n",
    "            self._update_gradient(grad_val=X.T)\n",
    "        return np.dot(X, self.w) # num_sample * output_dim\n",
    "    \n",
    "    def _update_gradient(self, grad_val):\n",
    "        self._gradient = grad_val\n",
    "\n",
    "    @property\n",
    "    def gradient(self):\n",
    "        return self._gradient # input_dim * num_samples\n",
    "\n",
    "    def _load_params(self, params):\n",
    "        self.w = params['w'].data.numpy().T\n",
    "        self.b = params['b'].data.numpy()\n",
    "        if self.bias:\n",
    "            self.w = np.vstack((self.w, self.b.reshape(1, -1)))\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        if self.bias:\n",
    "            self._parameters_dict = {'w': self.w[:-1, :], 'b': self.w[-1, :]}\n",
    "        else:\n",
    "            self._parameters_dict = {'w': self.w}\n",
    "        return self._parameters_dict\n",
    "    \n",
    "    def _update_params(self, update_value):\n",
    "        self.w -= update_value\n",
    "\n",
    "    def load(self, params_dict):\n",
    "        self.w = params_dict['w']\n",
    "        self.b = params_dict['b']\n",
    "        if self.bias:\n",
    "            self.w = np.vstack((self.w, self.b.reshape(1, -1)))\n",
    "            \n",
    "class ReLu():\n",
    "    def __init__(self, inplace=False):\n",
    "        self.inplace = inplace\n",
    "        self._gradient = None\n",
    "    \n",
    "    def __call__(self, X, update_grad=False):\n",
    "        if update_grad:\n",
    "            self._update_gradient(grad_val=(X > 0))\n",
    "        if self.inplace:\n",
    "            np.maximum(X, 0, X)\n",
    "        else:\n",
    "            X = X.copy()\n",
    "            return np.maximum(X, 0, X)\n",
    "    \n",
    "    def _update_gradient(self, grad_val):\n",
    "        self._gradient = grad_val\n",
    "\n",
    "    @property\n",
    "    def gradient(self):\n",
    "        return self._gradient\n",
    "\n",
    "class LeakyReLu():\n",
    "    def __init__(self, negative_slpoe=0.1, inplace=False):\n",
    "        self.negative_slope = negative_slpoe\n",
    "        self.inplace = inplace\n",
    "        self._gradient = None\n",
    "    \n",
    "    def __call__(self, X, update_grad=False):\n",
    "        if not self.inplace:\n",
    "            X = X.copy()\n",
    "        X[X > 0] = 1\n",
    "        X[X < 0] = -self.negative_slope\n",
    "        if update_grad:\n",
    "            self._update_gradient(grad_val=X)\n",
    "        if not self.inplace:\n",
    "            return X\n",
    "\n",
    "    def _update_gradient(self, grad_val):\n",
    "        self._gradient = grad_val\n",
    "    \n",
    "    @property\n",
    "    def gradient(self):\n",
    "        return self._gradient\n",
    "    \n",
    "    def _train(self):\n",
    "        self._train_mod = True\n",
    "    \n",
    "    def _eval(self):\n",
    "        self._train_mod = False\n",
    "        \n",
    "class CrossEntropLoss():\n",
    "    def __init__(self):\n",
    "       self._gradient = None\n",
    "       self._train_mod = False\n",
    "\n",
    "    def __call__(self, predicts, labels):\n",
    "        num_classes = predicts.shape[1]\n",
    "\n",
    "        transform_labels = np.eye(num_classes)[labels]\n",
    "        if num_classes > 2:\n",
    "            transform_predicts = np.exp(predicts) / np.exp(predicts).sum(axis=1, keepdims=1)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        res = -transform_labels * np.log(transform_predicts)\n",
    "        if self._train_mod:\n",
    "            self._update_gradient(grad_val=transform_predicts - transform_labels)\n",
    "        return np.sum(res) / res.shape[0]\n",
    "\n",
    "    def _update_gradient(self, grad_val):\n",
    "        self._gradient = grad_val\n",
    "\n",
    "    @property\n",
    "    def gradient(self):\n",
    "        return self._gradient # num_samples * num_class\n",
    "\n",
    "    @property\n",
    "    def train_mod(self):\n",
    "        return self._train_mod\n",
    "    \n",
    "    @train_mod.setter\n",
    "    def train_mod(self, flag):\n",
    "        self._train_mod = flag\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, loss_func):\n",
    "        self.layer1 = Linear(input_dim, hidden_dim)\n",
    "        self.relu = LeakyReLu(negative_slpoe=0.1)\n",
    "        self.layer2 = Linear(hidden_dim, output_dim)\n",
    "        self.loss_func = loss_func\n",
    "        self._parameters_dict = self.parameters\n",
    "        self._train_mod = False\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        X = self.layer1(X, update_grad=self._train_mod)\n",
    "        X = self.relu(X, update_grad=self._train_mod)\n",
    "        X = self.layer2(X, update_grad=self._train_mod)\n",
    "        return X\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        self._parameters_dict = dict((k, v.parameters) for (k, v) in self.__dict__.items() if hasattr(v, 'parameters'))\n",
    "        return self._parameters_dict\n",
    "    \n",
    "    # 当前版本未能实现计算图，所以需要手动编辑梯度\n",
    "    def backward(self, l2_weight=0):\n",
    "        self.gradient_dict = dict()\n",
    "        if self.layer2.bias:\n",
    "            self.gradient_dict['layer1'] = np.dot(self.layer1.gradient, self.relu.gradient * np.dot(self.loss_func.gradient, self.layer2.w[:-1, :].T))\n",
    "        else:\n",
    "            self.gradient_dict['layer1'] = np.dot(self.layer1.gradient, self.relu.gradient * np.dot(self.loss_func.gradient, self.layer2.w.T))\n",
    "        self.gradient_dict['layer2'] = np.dot(self.layer2.gradient, self.loss_func.gradient)\n",
    "\n",
    "        if self.layer1.bias:\n",
    "            self.gradient_dict['layer1'][:-1, :] += l2_weight * self.layer1.w[:-1, :]\n",
    "        else:\n",
    "            self.gradient_dict['layer1'] += l2_weight * self.layer1.w\n",
    "        \n",
    "        if self.layer2.bias:\n",
    "            self.gradient_dict['layer2'][:-1, :] += l2_weight * self.layer2.w[:-1, :]\n",
    "        else:\n",
    "            self.gradient_dict['layer2'] += l2_weight * self.layer2.w\n",
    "    \n",
    "    def sgd_step(self, lr=0.01, decay_rate=0.98, decay_step=1000, cur_step=0):\n",
    "        # 使用指数化的步长缩减策略\n",
    "        cur_lr = lr * decay_rate ** (cur_step / decay_step)\n",
    "        for layer, params in self.gradient_dict.items():\n",
    "            self.__dict__[layer]._update_params(cur_lr * params)\n",
    "    \n",
    "    def dump(self, path=None):\n",
    "        if path is None:\n",
    "            cur_path = os.getcwd()\n",
    "            file_path = os.path.join(cur_path, 'model')\n",
    "            os.makedirs(file_path, exist_ok=True)\n",
    "        else:\n",
    "            file_path = path\n",
    "\n",
    "        for cur_layer in self.parameters:\n",
    "            cur_layer_path = os.path.join(file_path, cur_layer)\n",
    "            os.makedirs(cur_layer_path, exist_ok=True)\n",
    "            for content in self.parameters[cur_layer]:\n",
    "                self.parameters[cur_layer][content].dump(os.path.join(cur_layer_path, f'{content}.npy'))\n",
    "            \n",
    "    def load(self, path):\n",
    "        param_dict = {}\n",
    "        for layer in os.listdir(path):\n",
    "            param_dict[layer] = dict()\n",
    "            layer_path = os.path.join(path, layer)\n",
    "            for content in os.listdir(layer_path):\n",
    "                content_name = content.split('.')[0]\n",
    "                layer_content = np.load(os.path.join(layer_path, content), allow_pickle=True)\n",
    "                param_dict[layer][content_name] = layer_content\n",
    "        for layer in param_dict:\n",
    "            self.__dict__[layer].load(param_dict[layer])\n",
    "    \n",
    "    def train(self):\n",
    "        self._train_mod = True\n",
    "        self.loss_func.train_mod = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self._train_mod = False\n",
    "        self.loss_func.train_mod = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [28:52<00:00, 11.54it/s]\n"
     ]
    }
   ],
   "source": [
    "loss_func = CrossEntropLoss()\n",
    "\n",
    "m = Model(input_dim=784, hidden_dim=64, output_dim=10, loss_func=loss_func)\n",
    "train_index = [_ for _ in range(len(train_images))]\n",
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "test_loss_list = []\n",
    "for epoch in tqdm(range(20000)):\n",
    "    m.train()\n",
    "    idx = np.random.choice(train_index)\n",
    "    input_x = train_images[idx].reshape(1, -1)\n",
    "    labels = train_labels[idx]\n",
    "    predicts = m(input_x)\n",
    "    \n",
    "    train_loss = loss_func(predicts, labels)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    m.eval()\n",
    "    valid_predict = m(valid_images)\n",
    "    test_predict = m(test_images)\n",
    "\n",
    "    valid_loss = loss_func(valid_predict, valid_labels)\n",
    "    test_loss = loss_func(test_predict, test_labels)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "    valid_acc = accuracy_score(valid_predict.argmax(axis=1), valid_labels)\n",
    "    test_acc = accuracy_score(test_predict.argmax(axis=1), test_labels)\n",
    "    valid_acc_list.append(valid_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    \n",
    "    m.backward(l2_weight=0.0001)\n",
    "    m.sgd_step(lr=0.05, cur_step=epoch, decay_step=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(model, dataset, label):\n",
    "    predicts = model(dataset).argmax(axis=1)\n",
    "    acc = accuracy_score(predicts, label)\n",
    "    prec = precision_score(predicts, label, average='macro')\n",
    "    rec = recall_score(predicts, label, average='macro')\n",
    "    f1 = f1_score(predicts, label, average='macro')\n",
    "    print(f\"Accuracy: {acc:.4f} Precision: {prec:.4f} Recall: {rec:.4f} F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8732 Precision: 0.8720 Recall: 0.8733 F1-Score: 0.8721\n",
      "Accuracy: 0.8732 Precision: 0.8718 Recall: 0.8731 F1-Score: 0.8719\n",
      "Accuracy: 0.8808 Precision: 0.8797 Recall: 0.8808 F1-Score: 0.8796\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(m, train_images, train_labels)\n",
    "calc_metrics(m, valid_images, valid_labels)\n",
    "calc_metrics(m, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数搜索\n",
    "# 首先对模型的训练过程进行封装\n",
    "def train_one_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_m = Model(input_dim=784, hidden_dim=64, output_dim=10, loss_func=loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_m.load('/Users/tsuilai/Desktop/课程代码/神经网络与深度学习/Assign1/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8808"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_test_predicts = copy_m(test_images).argmax(axis=1)\n",
    "np.sum(copy_test_predicts == test_labels) / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
